"""
LLM as a judge to judge the quality of the AI agent answers.

Step 1:
Load sample data:
 - questions are located at the folder `experiment_1_questions_with_context/`;
 Each question is a txt file, e.g. `question1.txt`;
 
 - `experiment_1_answers.json` contains the answers to the questions;
 It's of the format:
[
    {
        "question_with_context": "question.txt",
        “ai_reflection”: "reflection1",
        "ai_plan": "plan1",
        "human_reflection": "reflection2"
        "human_plan": "plan2"
    },
    ...
]


Step 2:
Invoke the judge llm to judge the responses;
prepare the prompt by:
```
prompt_input = [question_with_context, reflection_1, response_1, reflection_2, response_2]
prompt_template = config.agent_path + "/llm_modules/prompt_templates/llm_judge_v1.txt"
prompt = generate_prompt(prompt_input, prompt_template)
```
# Note: you must randomly permute the order of the ai and human answers; so LLM don't know which is which!

Store the results in a dictionary, named "judgements", with the following key, value pairs:
 - key: file name, like "question1.txt"
 - vale: "ai" | "human" | "tie"


Step 3:
Store the results in a file named "experiment_1_judgements.json"
format:
{
    "question1.txt": "ai" | "human" | "tie",
}
"""

import json
import os
import random
from gatsim import config
from gatsim.utils import extract_json_from_string
from gatsim.agent.llm_modules.llm import generate_prompt, llm_judge_generate


def main():
    # Step 1: Load sample data
    print("Step 1: Loading sample data...")
    
    # Load answers from JSON file
    with open('experiment_1_answers.json', 'r', encoding='utf-8') as f:
        responses_data = json.load(f)
    
    # Load questions from text files
    questions_folder = 'experiment_1_questions_with_context'
    questions = {}
    
    for response_entry in responses_data:
        question_file = response_entry['question_with_context']
        question_path = os.path.join(questions_folder, question_file)
        
        with open(question_path, 'r', encoding='utf-8') as f:
            questions[question_file] = f.read().strip()
    
    print(f"Loaded {len(responses_data)} question-answer pairs")
    
    
    # Step 2: Judge the answers
    print("Step 2: Judging answers...")
    
    judgements = {}
    prompt_template = config.agent_path + "/llm_modules/prompt_templates/llm_judge_v1.txt"
    
    for i, response_entry in enumerate(responses_data):
        question_file = response_entry['question_with_context']
        ai_reflection = response_entry['ai_reflection']
        ai_plan = response_entry['ai_plan']
        human_reflection = response_entry['human_reflection']
        human_plan = response_entry['human_plan']
        
        if ai_plan == human_plan:
            judgements[question_file] = 'tie'
            continue
        
        question_with_context = questions[question_file]
        
        print(f"Processing {question_file} ({i+1}/{len(responses_data)})")
        
        # Randomly permute the order of AI and human answers
        if random.random() < 0.5:
            reflection_1 = ai_reflection
            plan_1 = ai_plan
            reflection_2 = human_reflection
            plan_2 = human_plan
            ai_is_first = True
        else:
            reflection_1 = human_reflection
            plan_1 = human_plan
            reflection_2 = ai_reflection
            plan_2 = ai_plan
            ai_is_first = False
        
        # Prepare prompt
        prompt_input = [question_with_context, reflection_1, plan_1, reflection_2, plan_2]
        prompt_template = config.agent_path + "/llm_modules/prompt_templates/llm_judge_v1.txt"
        prompt = generate_prompt(prompt_input, prompt_template)
        
        # Get judgment from LLM
        try:
            output = llm_judge_generate(prompt)
            """ 
            Output format:
            {
                "output": "1" | "2" | "tie"
            }

            output explanation:
            1: the first answer is more likely to be generated by a real man
            2: the second answer is more likely to be generated by a real man
            tie: the two answers are equally likely to be generated by a real man
            """
            
            output_parsed = extract_json_from_string(output)
            judgment_result = output_parsed['output']
            
            # Convert judgment back to ai/human/tie based on the permutation
            if judgment_result == "tie":
                final_judgment = "tie"
            elif judgment_result == "1":
                # First answer was chosen as more human-like
                final_judgment = "human" if ai_is_first else "ai"
            elif judgment_result == "2":
                # Second answer was chosen as more human-like
                final_judgment = "ai" if ai_is_first else "human"
            else:
                print(f"Warning: Unexpected judgment result '{judgment_result}' for {question_file}")
                final_judgment = "tie"  # Default to tie for unexpected results
            
            judgements[question_file] = final_judgment
            print(f"  -> {final_judgment}")
            
        except Exception as e:
            print(f"Error processing {question_file}: {str(e)}")
            judgements[question_file] = "tie"  # Default to tie on error
    
    # Step 3: Store results
    print("Step 3: Storing results...")
    
    with open('experiment_1_judgements.json', 'w', encoding='utf-8') as f:
        json.dump(judgements, f, indent=2, ensure_ascii=False)
    
    # Print summary statistics
    ai_wins = sum(1 for v in judgements.values() if v == "ai")
    human_wins = sum(1 for v in judgements.values() if v == "human")
    ties = sum(1 for v in judgements.values() if v == "tie")
    total = len(judgements)
    
    print(f"\nSummary:")
    print(f"Total judgments: {total}")
    print(f"AI preferred: {ai_wins} ({ai_wins/total*100:.1f}%)")
    print(f"Human preferred: {human_wins} ({human_wins/total*100:.1f}%)")
    print(f"Ties: {ties} ({ties/total*100:.1f}%)")
    print(f"Results saved to experiment_1_judgements.json")


if __name__ == "__main__":
    main()
